{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  -q  tqdm\n",
    "!pip install -q torch torchvision\n",
    "!pip install -q pytorch-transformers\n",
    "!pip install -q sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_transformers import BertConfig,BertForSequenceClassification,BertTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from tqdm import tqdm_notebook  as tqdm\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "\n",
    "parser.add_argument(\"--max_seq_length\", default=384, type=int,\n",
    "                    help=\"The maximum total input sequence length after WordPiece tokenization. Sequences \"\n",
    "                         \"longer than this will be truncated, and sequences shorter than this will be padded.\")\n",
    "parser.add_argument(\"--max_content_length\", default=192, type=int,\n",
    "                    help=\"The maximum number of tokens for the content. Content longer than this will \"\n",
    "                         \"be truncated to this length.\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.0, type=float,\n",
    "                    help=\"Weight deay if we apply some.\")\n",
    "parser.add_argument(\"--warmup_steps\", default=0, type=int,\n",
    "                    help=\"Linear warmup over warmup_steps.\")\n",
    "parser.add_argument(\"--learning_rate\", default=5e-5, type=float,\n",
    "                    help=\"The initial learning rate for Adam.\")\n",
    "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float,\n",
    "                    help=\"Epsilon for Adam optimizer.\")\n",
    "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
    "                    help=\"Max gradient norm.\")\n",
    "parser.add_argument(\"--model_name\",default=\"bert-base-chinese\",type=str,\n",
    "                    help='bert pretrianed model')\n",
    "parser.add_argument(\"--train_batch_size\", default=8, type=int,\n",
    "                    help=\"Batch size for training.\")\n",
    "parser.add_argument(\"--eval_batch_size\", default=8, type=int,\n",
    "                    help=\"Batch size for evaluation.\")\n",
    "parser.add_argument(\"--num_train_epochs\", default=3.0, type=float,\n",
    "                    help=\"Total number of training epochs to perform.\")\n",
    "args = parser.parse_args('')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "args.device=device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 520/520 [00:00<00:00, 95421.01B/s]\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\"bert-base-chinese\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", config=config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert config.num_labels==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/data_with_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.content=data.content.fillna('')\n",
    "data.root_content=data.root_content.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>is_orig</th>\n",
       "      <th>content</th>\n",
       "      <th>root_content</th>\n",
       "      <th>wb_id</th>\n",
       "      <th>mobile</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5f2731634875ed63960e73e7b7318188</td>\n",
       "      <td>0</td>\n",
       "      <td>转发微博</td>\n",
       "      <td>柠檬水，你喝对了吗？ 不同的搭配有不同的功效哦，​ ​​​快来试试吧 ~ ​​</td>\n",
       "      <td>1746505677</td>\n",
       "      <td>18688728723</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1d4c605c286b8413fbda3da0855524ec</td>\n",
       "      <td>0</td>\n",
       "      <td>[爱你]</td>\n",
       "      <td>“微博第三个表情是你今年下半年的状态”  我是[爱你] ​​</td>\n",
       "      <td>1873781221</td>\n",
       "      <td>13837811122</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                uid  is_orig content  \\\n",
       "0  5f2731634875ed63960e73e7b7318188        0    转发微博   \n",
       "1  1d4c605c286b8413fbda3da0855524ec        0    [爱你]   \n",
       "\n",
       "                              root_content       wb_id       mobile  label  \n",
       "0  柠檬水，你喝对了吗？ 不同的搭配有不同的功效哦，​ ​​​快来试试吧 ~ ​​  1746505677  18688728723    1.0  \n",
       "1           “微博第三个表情是你今年下半年的状态”  我是[爱你] ​​  1873781221  13837811122    1.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>is_orig</th>\n",
       "      <th>content</th>\n",
       "      <th>root_content</th>\n",
       "      <th>wb_id</th>\n",
       "      <th>mobile</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43108</th>\n",
       "      <td>182f95d15b85539fb9209c1572eaf44e</td>\n",
       "      <td>1</td>\n",
       "      <td>#微博豪车抽奖# http:</td>\n",
       "      <td></td>\n",
       "      <td>6701158809</td>\n",
       "      <td>18479698848</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43109</th>\n",
       "      <td>182f95d15b85539fb9209c1572eaf44e</td>\n",
       "      <td>1</td>\n",
       "      <td>微博@借钱 居然抽奔驰，好想被宠幸哦，今年的好运都指着它了！为了豪车，冲呀！！#微博豪车抽奖...</td>\n",
       "      <td></td>\n",
       "      <td>6701158809</td>\n",
       "      <td>18479698848</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    uid  is_orig  \\\n",
       "43108  182f95d15b85539fb9209c1572eaf44e        1   \n",
       "43109  182f95d15b85539fb9209c1572eaf44e        1   \n",
       "\n",
       "                                                 content root_content  \\\n",
       "43108                                     #微博豪车抽奖# http:                \n",
       "43109  微博@借钱 居然抽奔驰，好想被宠幸哦，今年的好运都指着它了！为了豪车，冲呀！！#微博豪车抽奖...                \n",
       "\n",
       "            wb_id       mobile  label  \n",
       "43108  6701158809  18479698848    1.0  \n",
       "43109  6701158809  18479698848    1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,eval_data = train_test_split(data,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_ids,\n",
    "                 input_mask,\n",
    "                 segment_ids,\n",
    "                 label\n",
    "                ):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def convert_data(data,\n",
    "                 max_seq_length,\n",
    "                 max_content_length,\n",
    "                 sep_token='[SEP]',\n",
    "                 pad_token=0,\n",
    "                 sequence_a_segment_id=0,\n",
    "                 sequence_b_segment_id=1,\n",
    "                 pad_token_segment_id=0,\n",
    "                 mask_padding_with_zero=True\n",
    "                 ):\n",
    "    features = []\n",
    "    for content, root_content, label in tqdm_notebook(zip(data.content, data.root_content, data.label), total=len(data)):\n",
    "        content_tokens = tokenizer.tokenize(content)\n",
    "        root_content_tokens = tokenizer.tokenize(root_content)\n",
    "\n",
    "        if len(content_tokens) > max_content_length:\n",
    "            content_tokens = content_tokens[0:max_content_length]\n",
    "        # The -3 accounts for [SEP] and [SEP]\n",
    "        max_tokens_for_root_content = max_seq_length - len(content_tokens) - 2\n",
    "\n",
    "        if len(root_content_tokens) > max_tokens_for_root_content:\n",
    "            root_content_tokens = root_content_tokens[0:max_tokens_for_root_content]\n",
    "\n",
    "        tokens = []\n",
    "        segment_ids = []\n",
    "\n",
    "        # content\n",
    "        for token in content_tokens:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(sequence_a_segment_id)\n",
    "\n",
    "        # SEP token\n",
    "        tokens.append(sep_token)\n",
    "        segment_ids.append(sequence_a_segment_id)\n",
    "\n",
    "        # root_content\n",
    "        for token in root_content_tokens:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(sequence_b_segment_id)\n",
    "\n",
    "        # SEP token\n",
    "        tokens.append(sep_token)\n",
    "        segment_ids.append(sequence_b_segment_id)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(pad_token)\n",
    "            input_mask.append(0 if mask_padding_with_zero else 1)\n",
    "            segment_ids.append(pad_token_segment_id)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        features.append(InputFeatures(\n",
    "            input_ids, input_mask, segment_ids, label))\n",
    "\n",
    "    all_input_ids = torch.tensor(\n",
    "        [f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor(\n",
    "        [f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor(\n",
    "        [f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label = torch.tensor([f.label for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask,\n",
    "                            all_segment_ids, all_label)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0a4fab764e47d899bc44f965dc0317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28883), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = convert_data(train_data,max_seq_length=args.max_seq_length,max_content_length=args.max_content_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06dd24ef8a44e17a3610d51ca232777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14227), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset =  convert_data(eval_data,max_seq_length=args.max_seq_length,max_content_length=args.max_content_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "eval_sampler = SequentialSampler(eval_dataset) \n",
    "eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader)  * args.num_train_epochs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "model.zero_grad()\n",
    "\n",
    "for _ in tqdm(range(int(args.num_train_epochs)), desc=\"Epoch\"):\n",
    "    tr_acc=[]\n",
    "    tr_loss=[]\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        model.train()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        inputs = {'input_ids':       batch[0],\n",
    "                  'attention_mask':  batch[1], \n",
    "                  'token_type_ids':  batch[2],  \n",
    "                  'labels': batch[3], }\n",
    "        labels = batch[3]\n",
    "        loss,logits = model(**inputs)[:2]\n",
    "        _,indices=torch.max(logits,dim=1)\n",
    "        tr_acc.append(labels.numpy()==indices.numpy())\n",
    "        tr_loss.append(loss.numpy())\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        model.zero_grad()\n",
    "        \n",
    "        if step%(len(train_dataloader)//10)==0:\n",
    "            \n",
    "            print(f'train loss:{np.mean(tr_loss)} on {step} step')\n",
    "            print(f'train acc:{np.mean(tr_acc)} on {step} step')\n",
    "            tr_loss=[]\n",
    "            tr_acc=[]\n",
    "            \n",
    "            \n",
    "    tr_acc=[]\n",
    "    tr_loss=[]\n",
    "    tr_result=[]\n",
    "    print(\"***** Running evaluation *****\")\n",
    "    i=0\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(args.device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':       batch[0],\n",
    "                      'attention_mask':  batch[1], \n",
    "                      'token_type_ids':  batch[2],  \n",
    "                      'labels': batch[3], }\n",
    "            labels = batch[3]\n",
    "            loss,logits = model(**inputs)[:2]\n",
    "            _,indices=torch.max(logits,dim=1)\n",
    "            tr_acc.append(labels.numpy()==indices.numpy())\n",
    "            tr_loss.append(loss.numpy())\n",
    "            tr_result.append(indices.numpy())\n",
    "    print(f'eval loss: {np.mean(tr_loss)}')\n",
    "    print(f'eval acc: {np.mean(tr_acc)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
